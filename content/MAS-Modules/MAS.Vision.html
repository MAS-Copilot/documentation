<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>MAS.Vision 视觉 | DataMaster </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="MAS.Vision 视觉 | DataMaster ">
      
      
      <link rel="icon" href="../../images/favicon.ico">
      <link rel="stylesheet" href="../../public/docfx.min.css">
      <link rel="stylesheet" href="../../public/main.css">
      <meta name="docfx:navrel" content="../../toc.html">
      <meta name="docfx:tocrel" content="../toc.html">
      
      <meta name="docfx:rel" content="../../">
      
      
      <meta name="docfx:docurl" content="https://bitbucket.org/mas-software/industrial-mes-net-platform-advanced/src/main/docs/content/MAS-Modules/MAS.Vision.md#lines-1">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../../index.html">
            <img id="logo" class="svg" src="../../images/logo.svg" alt="DataMaster">
            DataMaster
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled="" placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">
<h1 id="masvision-视觉">MAS.Vision 视觉</h1>

<h2 id="opencv">OpenCV</h2>
<p><strong>相关资料：</strong></p>
<ul>
<li><code>OpenCV</code> 官方文档，提供了详尽的API文档和教程，是学习和应用OpenCV的重要资源： <a href="https://docs.opencv.org/4.x/index.html">OpenCV Documentation</a></li>
<li><code>OpenCV</code> 项目地址：<a href="https://github.com/opencv/opencv">OpenCV</a>，官网：<a href="https://opencv.org/">OpenCV Org</a></li>
<li><code>OpenCvSharp4</code> 在.NET环境下使用的 OpenCV 库的封装。项目地址：<a href="https://github.com/shimat/opencvsharp">OpenCvSharp</a></li>
<li>标注工具：<a href="https://github.com/cvat-ai/cvat">CVAT</a></li>
<li>部署容器：<a href="https://www.docker.com/products/docker-desktop/">Docker Desktop</a></li>
<li>目标检测算法模型：<a href="https://github.com/ultralytics/yolov5">yolov5</a></li>
<li>检查模型的输入和输出工具：<a href="https://github.com/lutzroeder/netron">Netron</a></li>
<li><code>Microsoft Azure Global Edition</code> 技术文档：<a href="https://learn.microsoft.com/zh-cn/azure/machine-learning/how-to-inference-onnx-automl-image-models?view=azureml-api-2&amp;tabs=multi-class#load-the-labels-and-onnx-model-files">ONNX</a></li>
<li>关于 <code>fbgemm.dll&quot; or one of its dependencies</code> 的问题：<a href="https://stackoverflow.com/questions/78786306/fbgemm-load-error-trying-to-use-pytorch-on-windows">Stackoverflow</a></li>
</ul>
<p><strong>以下因商业版弃用：</strong></p>
<ul>
<li><code>Emgu CV</code> .NET的 OpenCV 封装，C#和.NET环境下的应用程序使用 OpenCV 功能文档：<a href="https://www.emgu.com/wiki/index.php/Documentation">Emgu CV Wiki</a></li>
<li><code>Emgu CV</code> 项目地址：<a href="https://github.com/emgucv/emgucv">Emgu CV</a>，官网：<a href="https://www.emgu.com/wiki/index.php/Main_Page">Emgu CV Org</a></li>
</ul>
<h2 id="应用场景区分">应用场景区分</h2>
<table>
<thead>
<tr>
<th>功能/场景</th>
<th>目标检测算法模型 + OpenCV</th>
<th>仅 OpenCV</th>
</tr>
</thead>
<tbody>
<tr>
<td>算法复杂度</td>
<td>高，需要深度学习</td>
<td>高，传统图像处理</td>
</tr>
<tr>
<td>数据集</td>
<td>大量标注数据</td>
<td>可能不需要或数据量较少</td>
</tr>
<tr>
<td>对象检测</td>
<td>高精度检测多种对象，实时分析视频流</td>
<td>传统方法，如模板匹配、特征检测等</td>
</tr>
<tr>
<td>特征提取</td>
<td>可用于目标检测的特征提取</td>
<td>SIFT、SURF等传统特征提取</td>
</tr>
<tr>
<td>运动追踪</td>
<td>追踪视频中多个动态对象，高效识别对象变化</td>
<td>可进行简单运动追踪，如背景消除和光流法</td>
</tr>
<tr>
<td>人脸识别</td>
<td>精确识别并分析人脸特征，支持复杂场景</td>
<td>基本人脸检测，不含深度学习增强</td>
</tr>
<tr>
<td>物体分类</td>
<td>实时识别和分类多种对象类型，支持大规模数据集</td>
<td>不支持深度学习的分类，限于预定义或简单模式的识别</td>
</tr>
<tr>
<td>图像处理基础</td>
<td>完全支持</td>
<td>完全支持</td>
</tr>
<tr>
<td>动态场景分析</td>
<td>分析并理解视频内容中的复杂动态场景</td>
<td>有限的动态场景分析能力</td>
</tr>
<tr>
<td>处理速度与资源消耗</td>
<td>对硬件要求较高，需要更强的计算能力</td>
<td>较低的资源消耗，适用于资源受限环境</td>
</tr>
<tr>
<td>开发与集成复杂度</td>
<td>更高的开发难度，需要更多的调试和优化</td>
<td>相对简单，易于快速开发和部署</td>
</tr>
<tr>
<td>复杂场景</td>
<td>鲁棒性强，适用于复杂场景</td>
<td>对于复杂场景可能效果较差</td>
</tr>
</tbody>
</table>
<h2 id="训练基本流程">训练基本流程</h2>
<h3 id="样本准备">样本准备</h3>
<ul>
<li>需要收集到足够多的有关特定物体的图像数据</li>
<li>确保在不同的光照、角度和背景条件下拍摄，以提高模型的泛化能力</li>
</ul>
<h3 id="数据预处理">数据预处理</h3>
<ul>
<li>使用标注工具 <a href="https://github.com/cvat-ai/cvat">CVAT</a> 对图像中的物体进行标注，标出物体的边界框和类别</li>
<li>完成后导出格式 <code>YOLO</code></li>
</ul>
<h3 id="选择模型">选择模型</h3>
<ul>
<li><a href="https://github.com/ultralytics/yolov5">YOLOv5</a> 转到 <code>Releases</code> 选择模型下载</li>
</ul>
<p><img src="../../images/Vision/yolov5model.png" alt="yolov5model"></p>
<h3 id="模型训练">模型训练</h3>
<h4 id="打开命令行已安装python">打开命令行（已安装python）</h4>
<ul>
<li>安装 PyTorch</li>
</ul>
<pre><code class="lang-bash">pip install torch torchvision
</code></pre>
<ul>
<li>克隆 <code>YOLOv5</code> 仓库</li>
</ul>
<pre><code class="lang-bash">git clone https://github.com/ultralytics/yolov5
</code></pre>
<ul>
<li>进入项目根目录</li>
</ul>
<pre><code class="lang-bash">cd yolov5
</code></pre>
<ul>
<li>安装 <code>YOLOv5</code> 所需的其他依赖</li>
</ul>
<pre><code class="lang-bash">pip install -r requirements.txt
</code></pre>
<ul>
<li>创建图像和标注目录</li>
</ul>
<pre><code class="lang-bash">mkdir -p data/images/train/分类/
mkdir -p data/images/val/分类/
mkdir -p data/labels/train/分类/
mkdir -p data/labels/val/分类/
</code></pre>
<ul>
<li>移动训练数据（完成步骤 3，将文件移动到 yolov5 根目录执行以下命令）</li>
</ul>
<pre><code class="lang-bash">mv obj_Train_data/*.jpg data/images/train/分类/
mv obj_Train_data/*.txt data/labels/train/分类/
</code></pre>
<ul>
<li>移动验证数据</li>
</ul>
<pre><code class="lang-bash">mv obj_Validation_data/*.jpg data/images/val/分类/
mv obj_Validation_data/*.txt data/labels/val/分类/
</code></pre>
<h4 id="在-yolov5data-目录下创建一个名为-custom_datayaml-的文件并填写以下内容">在 <code>yolov5/data</code> 目录下，创建一个名为 <code>custom_data.yaml</code> 的文件，并填写以下内容</h4>
<pre><code class="lang-xml">train: data/images/train/分类/
val: data/images/val/分类/

nc: 2                       # 类别数，根据obj.names中定义的类别数量
names: ['类别1', '类别2']    # 根据obj.names文件中的类别名称修改
</code></pre>
<h4 id="开始训练">开始训练</h4>
<ul>
<li>--img 640 图像大小；--batch 批次；--epochs 训练周期；--weights 预训练权重；--single-cls 单类别检测，有助于优化训练过程</li>
</ul>
<pre><code class="lang-bash">python train.py --img 640 --batch 16 --epochs 50 --data data/custom_data.yaml --weights yolov5m.pt --single-cls
</code></pre>
<h4 id="训练过程中可以新建命令行导航到-yolov5-根目录执行以下命令">训练过程中，可以新建命令行，导航到 <code>yolov5</code> 根目录，执行以下命令</h4>
<ul>
<li>安装 tensorboard</li>
</ul>
<pre><code class="lang-bash">pip install tensorboard
</code></pre>
<h4 id="可视化训练过程中的各种指标然后启动在浏览器中访问这个地址来查看训练进度">可视化训练过程中的各种指标，然后启动，在浏览器中访问这个地址来查看训练进度</h4>
<pre><code class="lang-bash">tensorboard --logdir runs/train
</code></pre>
<p><strong>损失值:</strong></p>
<p><code>box_loss</code>: 定位损失，用于衡量模型预测的边界框与实际边界框的偏差</p>
<p><code>obj_loss</code>: 目标损失，用于衡量模型识别目标的能力</p>
<p><code>cls_loss</code>: 分类损失，用于分类任务</p>
<p><strong>指标：</strong></p>
<p><code>precision</code>: 精确率，用于衡量模型预测的正样本中实际为正样本的比例</p>
<p><code>recall</code>: 召回率，用于衡量模型对所有正样本的检测能力</p>
<p><code>mAP</code>: 平均精度，用于综合衡量模型的检测性能</p>
<h4 id="等待训练完成后将模型导出为-onnxopen-neural-network-exchange">等待训练完成后，将模型导出为 <code>ONNX（Open Neural Network Exchange）</code></h4>
<ul>
<li>用于优化和简化ONNX模型</li>
</ul>
<pre><code class="lang-bash">pip install onnx-simplifier
</code></pre>
<ul>
<li>执行导出</li>
</ul>
<pre><code class="lang-bash">python export.py --weights runs/train/exp2/weights/best.pt --img 640 --batch 1 --dynamic --simplify --include onnx
</code></pre>
<h3 id="模型测试与优化">模型测试与优化</h3>
<h4 id="使用导出的-onnx-模型在一些测试数据上运行检测以评估模型的性能和准确性通过-yolov5-的-detectpy-脚本进行指定-onnx-模型作为权重参数">使用导出的 <code>ONNX</code> 模型在一些测试数据上运行检测，以评估模型的性能和准确性，通过 <code>YOLOv5</code> 的 <code>detect.py</code> 脚本进行，指定 <code>ONNX</code> 模型作为权重参数</h4>
<ul>
<li>用于在不同设备上运行 ONNX 模型</li>
</ul>
<pre><code class="lang-bash">pip install onnxruntime
</code></pre>
<ul>
<li>运行检测脚本</li>
</ul>
<pre><code class="lang-bash">python detect.py --weights runs/train/exp2/weights/best.onnx --img 640 --conf 0.25 --source data/images/val
</code></pre>
<h4 id="视频流进行实时对象检测测试">视频流进行实时对象检测测试</h4>
<ul>
<li>如 <code>opencv-python</code> 如果未安装</li>
</ul>
<pre><code class="lang-bash">pip install opencv-python
</code></pre>
<ul>
<li>启动 <code>YOLOv5</code> 使用摄像头进行实时检测</li>
</ul>
<pre><code class="lang-bash">python detect.py --weights runs/train/exp2/weights/best.onnx --conf 0.25 --source 0
</code></pre>
<h4 id="对训练好的模型进行剪枝量化等优化减小模型大小稍微提高推理速度">对训练好的模型进行剪枝、量化等优化，减小模型大小，稍微~提高推理速度</h4>
<h3 id="模型部署">模型部署</h3>
<ul>
<li>加载训练好的 <code>ONNX（Open Neural Network Exchange）</code> 模型，集成到.NET应用程序中，使其能够对新的图像进行实时或离线预测</li>
</ul>
<h2 id="设计需求">设计需求</h2>
<h3 id="v2024-08-15">v2024-08-15</h3>
<p><strong>任务识别及技术实现：</strong></p>
<p>1.对于物体方向的确定</p>
<ul>
<li><code>边缘检测</code> 为后续的轮廓分析和模板匹配提供基础，强调物体的结构边界</li>
<li><code>轮廓分析</code> 通过找到物体的最小外接矩形，可以通过矩形的长宽比和角度来判断物体是否处于正确的方向</li>
<li><code>几何形状匹配</code> 物体有特定的预期形状，通过匹配来确定其方向、分析图像的几何属性比较轮廓与预设模板之间的相似度</li>
<li><code>方向梯度直方图</code> 更复杂的物体识别和方向判断，可以提取特征向量来描述物体在各个方向上的边缘强度和方向</li>
<li><code>模板匹配</code> 基于整个图像的像素强度进行匹配，比较当前图像与模板的相似度</li>
</ul>
<h3 id="v2024-07-31">v2024-07-31</h3>
<h4 id="概述">概述</h4>
<p><strong>主要任务识别：</strong></p>
<p>1.在传送带上经过的物体</p>
<ul>
<li>物体类型</li>
<li>物体方向</li>
<li>计数递增</li>
<li>物体数量</li>
</ul>
<p>例如：手机、鸡蛋、电池</p>
<p><strong>预期效果：</strong></p>
<p><img src="../../images/Vision/objectDetection.png" alt="objectDetection"></p>
<h4 id="相机参数">相机参数</h4>
<p><strong>分辨率：</strong></p>
<p>二选一</p>
<ul>
<li><code>1080p(1920x1080（Full HD）)</code> 可以确保足够的图像细节，但会增加计算量</li>
<li><code>720p(1280x720（HD）)</code> 可以保证图像的清晰度，又能加快图像处理速度</li>
</ul>
<p><strong>帧率：</strong></p>
<ul>
<li>不低于 <code>30 fps</code>，每秒至少可以处理 <code>30</code> 张图片，满足一般生产线的速度要求</li>
</ul>
<p><strong>感光元件：</strong></p>
<ul>
<li>一般1/2.8英寸或1/3英寸的 <code>CMOS</code> 感光元件已经足够用于上述任务，能够在保证成本的同时提供良好的图像质量</li>
</ul>
<p><strong>接口类型：</strong></p>
<ul>
<li><code>USB 3.0</code> 提供高数据传输速率，适合高分辨率和高帧率的图像传输，且普遍兼容性好</li>
</ul>
<p><strong>镜头焦距：</strong></p>
<ul>
<li>根据拍摄距离选择，<code>35mm</code> 或 <code>50mm</code> 适用于 1-2 米的距离，<code>16mm</code> 至 <code>25mm</code> 适用于近距离（30cm至1m）拍摄</li>
</ul>
<p><strong>光圈：</strong></p>
<ul>
<li>随意或者默认，如果场景的照明条件变化莫测就要考虑</li>
</ul>
<p><strong>触发模式：</strong></p>
<p>非必需，可按需选择</p>
<ul>
<li>无触发模式，开启相机即实时捕获检测</li>
<li>有触发模式，软硬件触发拍摄，可以进一步提高图像捕捉的精确性</li>
</ul>
<p><strong>软件兼容性：</strong></p>
<ul>
<li>Windows 平台</li>
</ul>

</article>

        <div class="contribution d-print-none">
          <a href="https://bitbucket.org/mas-software/industrial-mes-net-platform-advanced/src/main/docs/content/MAS-Modules/MAS.Vision.md#lines-1" class="edit-link">Edit this page</a>
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>

      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          <span>Made with <a href="https://dotnet.github.io/docfx" rel="noreferrer">docfx</a>,  | Copyright © 2024 MAS(厦门威光). All rights reserved.</span>
        </div>
      </div>
    </footer>
  </body>
</html>
